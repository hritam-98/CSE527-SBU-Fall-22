{"cells":[{"cell_type":"markdown","source":["# CSE527 Homework 5 - 2\n","**Due date: 11:59 pm EST on Dec. 1, 2022 (Thu.)**\n","\n","In this semester, we will use Google Colab for the assignments, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You will need to use your Stony Brook (*.stonybrook.edu) account for coding and Google Drive to save your results."],"metadata":{"id":"JBlmFTi9y1wG"}},{"cell_type":"markdown","metadata":{"id":"bIt6veWXyU78"},"source":["Reading for HW5\n","--------------------------------------------------------------------------------\n","This time, to understand the task and network we are using, you need to\n","understand some papers from Google research and our nice colleagues:\n","\n","Understand counting: https://www3.cs.stonybrook.edu/~minhhoai/papers/fewshot_counting_CVPR21.pdf\n","\n","understand attention: https://www.youtube.com/watch?v=ptuGllU5SQQ\n","Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 9 - Self- Attention and Transformers\n","\n","understand ViT: https://github.com/google-research/vision_transformer\n","[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n","\n","Few-shot counting using Vision Transformer\n","--------------------------------------------------------------------------------\n","\n","In this programing assignment, we will train a Network to\n","count objects in a image.\n","Find the dataset here:\n","https://drive.google.com/file/d/1QVr5_wRTiRiCDKRVaN_bdb3Rm_IJaD0I/view?usp=sharing\n","Find the pretrained model parameter here:\n","https://drive.google.com/file/d/1PujXdKtUAVdrCaEeeTorFO5G7trWvs8S/view?usp=sharing\n","\n","\n","Put the compressed file inside the data folder, uncompress it.\n","You will see following structure\n","- gt_density_map_adaptive_384_VarV2\n","- json_annotationCombined_384_VarV2.json\n","- Train_Test_Val_FSC_147.json\n","- images_384_VarV2\n","\n","The two folders contain 6,146 images and labels and two json files for data precessing, you will not need to modify the data preprocessing part\n","\n","This is an experimental model. Your tasks are:\n","1. Most parts of the model are completed except the \\\\\n","  **Fully Connected Layer (MlpBlock - part I)** (20 points), \\\\\n","  the **Attention (EncoderBlock - part II )** (30 points),  \\\\\n","  and the **Transformer (Encoder - part III)** (30 points). \\\\\n","  You need to implement these modules exactly as described in order to load the pre-trained matrix. \n","2. Because the whole ViT Encoder is heavy to finetune with Colab, you will need to modify the code so that **the last 3 encoder layers of the ViT model are not included in the Transformer**. (It is fine if the performance is bad since you will be deleting 3 layers).\n","\n","Note 0: refer to doc/More_Details.pdf for details   \\\\\n","Note 1: You need to copy your code into lib/VIT.py, it has identical structure with this colab notebook \\\\\n","Note 2: The model has a long training time, let it run overnight and see what's the best loss value you can get. \\\\\n","HINT:\n","```shell\n","    (11): EncoderBlock(\n","      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (attn): SelfAttention(\n","        (query): LinearGeneral()\n","        (key): LinearGeneral()\n","        (value): LinearGeneral()\n","        (out): LinearGeneral()\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (mlp): MlpBlock(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (act): GELU()\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","```\n","\n","![img](./doc/model.JPG)"]},{"cell_type":"code","metadata":{"id":"yjb8FPvfyU8A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669505879136,"user_tz":300,"elapsed":16531,"user":{"displayName":"Hritam Basak","userId":"08916281092621863451"}},"outputId":"cafa4903-b57c-4db5-d77d-62b33eda4604"},"source":["\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"GwzN_dX-yU8B"},"source":["# Load Data(Unzipping everytime can increase the speed)"]},{"cell_type":"code","metadata":{"id":"I0E1R_Y3yU8C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669505880032,"user_tz":300,"elapsed":901,"user":{"displayName":"Hritam Basak","userId":"08916281092621863451"}},"outputId":"3a14bed6-d8bc-4a16-d5ed-35a93c416f58"},"source":["os.chdir('/content/gdrive/My Drive/Colab Notebooks/CV_2022Fall_HW5/data/')\n","#!ls\n","#!unzip CV_HW5_data.zip\n","\n","os.chdir('/content/gdrive/My Drive/Colab Notebooks/CV_2022Fall_HW5/')\n","!/opt/bin/nvidia-smi\n","!ls"],"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Nov 26 23:37:58 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   61C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","data\n"]}],"execution_count":null},{"cell_type":"code","metadata":{"id":"ySR4ItAdyU8C"},"source":["\n","\"\"\"\n","# Counting Transformer Model\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","# Position Embedding in the VIT\n","class PositionEmbs(nn.Module):\n","    def __init__(self, Max_num_patches, emb_dim, dropout_rate=0.1):\n","        super(PositionEmbs, self).__init__()\n","        self.pos_embedding = nn.Parameter(torch.randn(1, Max_num_patches, emb_dim))\n","        if dropout_rate > 0:\n","            self.dropout = nn.Dropout(dropout_rate)\n","        else:\n","            self.dropout = None\n","\n","    def forward(self, x):\n","        patches = x.shape[0]\n","        pos_embedding = self.pos_embedding.squeeze()\n","        out = x + pos_embedding[0:patches]\n","\n","        if self.dropout:\n","            out = self.dropout(out)\n","        out = out.unsqueeze(0)\n","        return out\n","\n","\n","class MlpBlock(nn.Module):\n","    \"\"\" Transformer Feed-Forward Block \"\"\"\n","    def __init__(self, in_dim, mlp_dim, out_dim, dropout_rate=0.1):\n","        super(MlpBlock, self).__init__()\n","        \"\"\"\n","            PART I: 20 points\n","            FC layer(with dropout) + GELU act layer + FC layer(with dropout)\n","        \"\"\"\n","        \"\"\" STUDENT CODE START \"\"\"\n","        self.fc1 = nn.Linear(in_dim, mlp_dim)\n","        self.fc2 = nn.Linear(mlp_dim, out_dim)\n","        self.activation = nn.GELU()\n","        self.dropout1 = nn.Dropout(dropout_rate)\n","        self.dropout2 = nn.Dropout(dropout_rate)\n","        \"\"\" STUDENT CODE END \"\"\"\n","\n","    def forward(self, x):\n","\n","        \"\"\" STUDENT CODE START \"\"\"\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = self.activation(x)\n","        #x = self.dropout(x)\n","        x = self.dropout1(x)\n","        out = self.dropout2(x)\n","        \"\"\" STUDENT CODE END \"\"\"\n","        return out\n","\n","# For implement the self-attention\n","class LinearGeneral(nn.Module):\n","    def __init__(self, in_dim=(768,), feat_dim=(12, 64)):\n","        super(LinearGeneral, self).__init__()\n","\n","        self.weight = nn.Parameter(torch.randn(*in_dim, *feat_dim))\n","        self.bias = nn.Parameter(torch.zeros(*feat_dim))\n","\n","    def forward(self, x, dims):\n","        a = torch.tensordot(x, self.weight, dims=dims) + self.bias\n","        return a\n","\n","# Multi-head attention\n","class SelfAttention(nn.Module):\n","    def __init__(self, in_dim, heads=8, dropout_rate=0.1):\n","        super(SelfAttention, self).__init__()\n","\n","        \"\"\"\n","            PART II: 40 points\n","            multihead SelfAttention part with Key, Query, Value\n","            use LinearGeneral class\n","        \"\"\"\n","        \"\"\" STUDENT CODE BEGIN \"\"\"\n","        self.head = heads\n","        self.head_dimension = in_dim // heads\n","        self.scale = np.sqrt(self.head_dimension)\n","\n","        self.q = LinearGeneral((in_dim,), (self.head, self.head_dimension))\n","        self.k = LinearGeneral((in_dim,), (self.head, self.head_dimension))\n","        self.v = LinearGeneral((in_dim,), (self.head, self.head_dimension))\n","        self.out = LinearGeneral((self.head, self.head_dimension), (in_dim, ))\n","        self.droput = nn.Dropout(dropout_rate)\n","        self.cluster = nn.Sequential(nn.Linear(297216, 256 * 100),\n","                                         nn.LeakyReLU(0.2),\n","                                         nn.Linear(256 * 100, 8 * 100))\n","\n","        \"\"\" STUDENT CODE END\"\"\"\n","\n","    def forward(self, x):\n","\n","        \"\"\" STUDENT CODE BEGIN \"\"\"\n","        b, n, _ = x.shape\n","        q = self.q(x, dims=([2], [0]))\n","        print(q.shape)\n","        q = self.cluster(q.view(b, -1)).view(b, 8, 1, 100)\n","        q = q.permute(0, 2, 1, 3)\n","        k = self.k(x, dims=([2], [0]))\n","        k = k.permute(0, 2, 1, 3)\n","        v = self.v(x, dims=([2], [0]))\n","        v = v.permute(0, 2, 1, 3)\n","\n","        wt = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n","        wt = F.softmax(wt, dim=-1)\n","\n","        out = torch.matmul(wt, v)\n","        out = out.permute(0, 2, 1, 3)\n","        out = self.out(out, dims=([2, 3], [0, 1]))\n","\n","        \"\"\" STUDENT CODE END \"\"\"\n","        return out\n","\n","# Encoder LayerNorm -> Self-attention -> Layernorm -> MLP\n","class EncoderBlock(nn.Module):\n","    def __init__(self, in_dim, mlp_dim, num_heads, dropout_rate=0.1, attn_dropout_rate=0.1):\n","        super(EncoderBlock, self).__init__()\n","\n","        self.norm1 = nn.LayerNorm(in_dim)\n","        self.attn = SelfAttention(in_dim, heads=num_heads, dropout_rate=attn_dropout_rate)\n","        if dropout_rate > 0:\n","            self.dropout = nn.Dropout(dropout_rate)\n","        else:\n","            self.dropout = None\n","        self.norm2 = nn.LayerNorm(in_dim)\n","        self.mlp = MlpBlock(in_dim, mlp_dim, in_dim, dropout_rate)\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.norm1(x)\n","        out = self.attn(out)\n","        if self.dropout:\n","            out = self.dropout(out)\n","        out += residual\n","        residual = out\n","\n","        out = self.norm2(out)\n","        out = self.mlp(out)\n","        out += residual\n","        return out\n","\n","# 12 Transformer layers\n","class Encoder(nn.Module):\n","    def __init__(self, emb_dim, mlp_dim, num_layers=12, num_heads=12, dropout_rate=0.1, attn_dropout_rate=0.0, Max_num_patches=512):\n","        super(Encoder, self).__init__()\n","\n","        # positional embedding\n","        self.pos_embedding = PositionEmbs(Max_num_patches, emb_dim, dropout_rate)\n","\n","        # encoder blocks\n","        in_dim = emb_dim\n","        self.encoder_layers = nn.ModuleList()\n","        \"\"\"\n","            PART III: 20 points\n","            Apply 12 layer of transformer and one LyaerNorm\n","        \"\"\"\n","        \"\"\" STUDENT CODE START \"\"\"\n","        for i in range(num_layers):\n","            l = EncoderBlock(in_dim, mlp_dim, num_heads, dropout_rate, attn_dropout_rate)\n","            self.encoder_layers.append(l)\n","        self.norm = nn.LayerNorm(in_dim)\n","        \"\"\" STUDENT CODE END \"\"\"\n","\n","\n","    def forward(self, x):\n","\n","        out = self.pos_embedding(x)\n","\n","        \"\"\" STUDENT CODE START \"\"\"\n","        for l in self.encoder_layers:\n","            out = l(out)\n","        out = self.norm(out)\n","        \"\"\" STUDENT CODE END \"\"\"\n","        return out\n","\n","# Upsampling and conv\n","class Decoder(nn.Module):\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","\n","        self.decoder = nn.Sequential(\n","            nn.Conv2d(768, 96, 7, padding=3),\n","            nn.ReLU(),\n","\n","            nn.UpsamplingBilinear2d(scale_factor=2),\n","            nn.Conv2d(96, 64, 5, padding=2),\n","            nn.ReLU(),\n","            nn.UpsamplingBilinear2d(scale_factor=2),\n","            nn.Conv2d(64, 32, 3, padding=1),\n","            nn.ReLU(),\n","            nn.UpsamplingBilinear2d(scale_factor=2),\n","            nn.Conv2d(32, 1, 1),\n","            nn.ReLU(),\n","            nn.UpsamplingBilinear2d(scale_factor=2),\n","        )\n","    # 4 * 4 * 768 -> 8 * 8 * 96 ->\n","    def forward(self, input, H, W, B):\n","        input = input.reshape(B, -1, H, W)\n","        out = self.decoder(input)\n","        return out\n","\n","class CountingTransformer(nn.Module):\n","    def __init__(self,\n","                 patch_size=(16, 16),\n","                 emb_dim=768,\n","                 mlp_dim=3072,\n","                 num_heads=12,\n","                 num_layers=12,\n","                 attn_dropout_rate=0.0,\n","                 dropout_rate=0.1,\n","                 feat_dim=None,\n","                 Max_num_patches=2000):\n","        super(CountingTransformer, self).__init__()\n","        # embedding layer\n","        self.patch_size = patch_size\n","        fh, fw = self.patch_size\n","\n","        # Image_patch -> token\n","        self.embedding = nn.Conv2d(3, emb_dim, kernel_size=(fh, fw), stride=(fh, fw))\n","\n","        # image_patch -> token\n","\n","        # Encoder\n","        self.transformer = Encoder(\n","            emb_dim=emb_dim,\n","            mlp_dim=mlp_dim,\n","            num_layers=num_layers,\n","            num_heads=num_heads,\n","            dropout_rate=dropout_rate,\n","            attn_dropout_rate=attn_dropout_rate,\n","            Max_num_patches=Max_num_patches)\n","\n","        # Decoder\n","        self.decoder = Decoder()\n","\n","    # Image and positive patches\n","    def forward(self, x, positive_token):\n","\n","        # x image\n","        # pos exmplar\n","\n","        # Image shape and patch size\n","        imh = x.shape[2]\n","        imw = x.shape[3]\n","        fh, fw = self.patch_size\n","\n","        # Patch num\n","        patch_num = int((imh / fh) * (imw / fw))\n","\n","        # H, W is the size to reshape at the decoder\n","        H = int(x.shape[2] / fh)\n","        W = int(x.shape[3] / fw)\n","        B = x.shape[0]\n","\n","        # (H * W) * token_dim -> H * W * C(token_dim)\n","        # 32 * 600 -> 8 * 4 * 600\n","\n","        # Embed a image patch into a token\n","        emb = self.embedding(x)     # (n, c, gh, gw)\n","        # Embed positive exemplar\n","        pos_emb = self.embedding(positive_token)\n","\n","        # Change the shape\n","        emb = emb.permute(0, 2, 3, 1)  # (n, gh, hw, c)\n","        b, h, w, c = emb.shape\n","        emb = emb.reshape(b, h * w, c)\n","        emb = emb.squeeze()\n","        pos_emb = pos_emb.squeeze()\n","        emb = torch.cat((emb, pos_emb))\n","\n","        # Encoder with image patches and pos exemplar\n","        feat = self.transformer(emb)\n","\n","        # Drop the exemplar embedding\n","        feat_throw_pos = feat[0,0:patch_num,:]\n","\n","        # Density map\n","        density_map = self.decoder(feat_throw_pos, H, W, B)\n","\n","        return density_map\n","\n","\n","#Check the pipeline\n","model = CountingTransformer(num_layers=12)\n","x = torch.randn((1, 3, 384, 256))\n","token = torch.randn((3, 3, 16, 16))\n","out = model(x, token)\n","print(out.shape)\n"],"outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"D78w4UFWyU8E"},"source":["# Training\n","## Dataloader and Model"]},{"cell_type":"code","metadata":{"id":"jQXxaYhmyU8E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669502079017,"user_tz":300,"elapsed":2414,"user":{"displayName":"Hritam Basak","userId":"08916281092621863451"}},"outputId":"a07a9705-9490-4c93-d791-073e296a0d48"},"source":["%cd /content/gdrive/MyDrive/Basak_Hritam_114783055_HW5_2\n","import cv2\n","import json\n","import copy\n","import os\n","import torch\n","import datetime\n","import numpy as np\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from lib.dataset import FscBgDataset\n","\n","os.chdir('/content/gdrive/My Drive/Colab Notebooks/CV_2022Fall_HW5/')\n","DATA_DIR = '/content/gdrive/My Drive/Colab Notebooks/CV_2022Fall_HW5/data/'\n","BATCH_SIZE = 16\n","PATCH_SIZE = (16,16)\n","Max_num_patches = 2500\n","LR = 5e-6\n","if torch.cuda.is_available():\n","    device = \"cuda:0\"\n","else:\n","    device = 'cpu'"],"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Basak_Hritam_114783055_HW5_2\n"]}],"execution_count":null},{"cell_type":"code","metadata":{"id":"_mcIyITTyU8E"},"source":["\n","from torch.utils.data import DataLoader\n","train_dataset = FscBgDataset(DATA_DIR, 'train')\n","val_dataset = FscBgDataset(DATA_DIR, 'val')\n","test_dataset = FscBgDataset(DATA_DIR, 'test')\n","im_idx = np.arange(len(train_dataset))\n","train_loader = DataLoader(im_idx, shuffle=True, batch_size=BATCH_SIZE)\n","im_idx = np.arange(len(val_dataset))\n","val_loader = DataLoader(im_idx, shuffle=True, batch_size=BATCH_SIZE)\n","im_idx = np.arange(len(test_dataset))\n","test_loader = DataLoader(im_idx, shuffle=True, batch_size=BATCH_SIZE)"],"outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"hEa5Lkn-yU8F","colab":{"base_uri":"https://localhost:8080/","height":963},"executionInfo":{"status":"error","timestamp":1669502132710,"user_tz":300,"elapsed":7628,"user":{"displayName":"Hritam Basak","userId":"08916281092621863451"}},"outputId":"c26a7fa4-0b3e-4a88-9853-1594bd83eeba"},"source":["\n","from lib.VIT import VisionTransformer\n","pretrain_path = '/content/gdrive/MyDrive/Basak_Hritam_114783055_HW5_2/model_save/imagenet21k+imagenet2012_ViT-B_16.pth'\n","pretrain_model = VisionTransformer(\n","          image_size=(384, 384),\n","          patch_size=(16, 16),\n","          emb_dim=768,\n","          mlp_dim=3072,\n","          num_heads=12,\n","          num_layers=12,\n","          num_classes=1000,\n","          attn_dropout_rate=0,\n","          dropout_rate=0.1)\n","\n","#Load pre-trained VIT backbone\n","sdict = torch.load(pretrain_path)\n","pretrain_model.load_state_dict(sdict['state_dict'])\n","\n","\n","#Counting Transformer\n","model = CountingTransformer(num_layers=12, Max_num_patches=Max_num_patches)\n","model.transformer.encoder_layers.load_state_dict(pretrain_model.transformer.encoder_layers.state_dict())\n","model.to(device)\n","criterion = nn.MSELoss().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=LR)"],"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-33c29094c4d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#Load pre-trained VIT backbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0msdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpretrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1605\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VisionTransformer:\n\tMissing key(s) in state_dict: \"transformer.encoder_layers.0.attn.q.weight\", \"transformer.encoder_layers.0.attn.q.bias\", \"transformer.encoder_layers.0.attn.k.weight\", \"transformer.encoder_layers.0.attn.k.bias\", \"transformer.encoder_layers.0.attn.v.weight\", \"transformer.encoder_layers.0.attn.v.bias\", \"transformer.encoder_layers.1.attn.q.weight\", \"transformer.encoder_layers.1.attn.q.bias\", \"transformer.encoder_layers.1.attn.k.weight\", \"transformer.encoder_layers.1.attn.k.bias\", \"transformer.encoder_layers.1.attn.v.weight\", \"transformer.encoder_layers.1.attn.v.bias\", \"transformer.encoder_layers.2.attn.q.weight\", \"transformer.encoder_layers.2.attn.q.bias\", \"transformer.encoder_layers.2.attn.k.weight\", \"transformer.encoder_layers.2.attn.k.bias\", \"transformer.encoder_layers.2.attn.v.weight\", \"transformer.encoder_layers.2.attn.v.bias\", \"transformer.encoder_layers.3.attn.q.weight\", \"transformer.encoder_layers.3.attn.q.bias\", \"transformer.encoder_layers.3.attn.k.weight\", \"transformer.encoder_layers.3.attn.k.bias\", \"transformer.encoder_layers.3.attn.v.weight\", \"transformer.encoder_layers.3.attn.v.bias\", \"transformer.encoder_layers.4.attn.q.weight\", \"transformer.encoder_layers.4.attn.q.bias\", \"transformer.encoder_layers.4.attn.k.weight\", \"transformer.encoder_layers.4.attn.k.bias\", \"transformer.encoder_layers.4.attn.v.weight\", \"transformer.encoder_layers.4.attn.v.bias\", \"transformer.encoder_layers.5.attn.q.weight\", \"transformer.encoder_layers.5.attn.q.bias\", \"transformer.encoder_layer...\n\tUnexpected key(s) in state_dict: \"transformer.encoder_layers.0.attn.key.bias\", \"transformer.encoder_layers.0.attn.key.weight\", \"transformer.encoder_layers.0.attn.query.bias\", \"transformer.encoder_layers.0.attn.query.weight\", \"transformer.encoder_layers.0.attn.value.bias\", \"transformer.encoder_layers.0.attn.value.weight\", \"transformer.encoder_layers.1.attn.key.bias\", \"transformer.encoder_layers.1.attn.key.weight\", \"transformer.encoder_layers.1.attn.query.bias\", \"transformer.encoder_layers.1.attn.query.weight\", \"transformer.encoder_layers.1.attn.value.bias\", \"transformer.encoder_layers.1.attn.value.weight\", \"transformer.encoder_layers.2.attn.key.bias\", \"transformer.encoder_layers.2.attn.key.weight\", \"transformer.encoder_layers.2.attn.query.bias\", \"transformer.encoder_layers.2.attn.query.weight\", \"transformer.encoder_layers.2.attn.value.bias\", \"transformer.encoder_layers.2.attn.value.weight\", \"transformer.encoder_layers.3.attn.key.bias\", \"transformer.encoder_layers.3.attn.key.weight\", \"transformer.encoder_layers.3.attn.query.bias\", \"transformer.encoder_layers.3.attn.query.weight\", \"transformer.encoder_layers.3.attn.value.bias\", \"transformer.encoder_layers.3.attn.value.weight\", \"transformer.encoder_layers.4.attn.key.bias\", \"transformer.encoder_layers.4.attn.key.weight\", \"transformer.encoder_layers.4.attn.query.bias\", \"transformer.encoder_layers.4.attn.query.weight\", \"transformer.encoder_layers.4.attn.value.bias\", \"transformer.encoder_layers.4.attn.value.weight\", \"transformer....\n\tsize mismatch for transformer.pos_embedding.pos_embedding: copying a param with shape torch.Size([1, 577, 768]) from checkpoint, the shape in current model is torch.Size([1, 513, 768])."]}],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"AEUt7tahyU8F"},"source":["## Train and test"]},{"cell_type":"code","metadata":{"id":"3-__VKUeyU8G","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1669502761617,"user_tz":300,"elapsed":645,"user":{"displayName":"Hritam Basak","userId":"08916281092621863451"}},"outputId":"973212af-a427-4888-c124-20d0ef045872"},"source":["from lib.utils import TransformTrain\n","\n","def train():\n","    SSE = 0\n","    SAE = 0\n","    train_mae = 0\n","    train_rmse = 0\n","    train_loss = 0\n","    cnt = 0\n","    starttime = datetime.datetime.now()\n","    for idx_batch in train_loader:\n","      optimizer.zero_grad()\n","      if debug:\n","          if cnt > 10:\n","            break\n","      for idx in idx_batch:\n","        cnt += 1\n","        train_sample = train_dataset[idx]\n","        im_id, image, boxes, dots, bg_mask_img, density = train_sample['im_id'], train_sample['image'], train_sample['boxes'], train_sample['dots'], train_sample['bg_mask_img'], train_sample['gt_density']\n","        sample = {'image': image, 'lines_boxes': boxes, 'gt_density': density}\n","        sample = TransformTrain(sample)\n","        image, boxes, GT_density = sample['image'].to(device), sample['boxes'].to(device), sample['gt_density'].to(device)\n","        boxes = boxes.squeeze()\n","\n","        # Get Positive Token Normal Scale\n","        Positive_Token = None\n","        for box_idx in range(boxes.shape[0]):\n","          box = boxes[box_idx]\n","          temp_Positive_Token = image[:,int(box[1]):int(box[3]),int(box[2]):int(box[4])]\n","          temp_Positive_Token = temp_Positive_Token.unsqueeze(0)\n","\n","          # reshape interpolate\n","          temp_Positive_Token = F.interpolate(temp_Positive_Token, size=(PATCH_SIZE[0], PATCH_SIZE[1]), mode='bilinear')\n","          if Positive_Token is None:\n","            Positive_Token = temp_Positive_Token\n","          else:\n","            Positive_Token = torch.cat((Positive_Token, temp_Positive_Token))\n","\n","        # Get scaling pos token\n","        scaling_para = [0.8, 0.9, 1/0.9, 1/0.8]\n","        for scaling in scaling_para:\n","          scaled_boxes = boxes / scaling\n","          scaled_boxes = scaled_boxes.squeeze()\n","          scaled_boxes[:, 1:3] = torch.floor(scaled_boxes[:, 1:3])\n","          scaled_boxes[:, 3:5] = torch.ceil(scaled_boxes[:, 3:5])\n","          scaled_boxes[:, 3:5] = scaled_boxes[:, 3:5] + 1\n","          scaled_boxes[:, 3] = torch.clamp_max(scaled_boxes[:, 3], image.shape[1] - 1)\n","          scaled_boxes[:, 4] = torch.clamp_max(scaled_boxes[:, 4], image.shape[2] - 1)\n","          scaled_boxes[:, 1:3] = torch.clamp_min(scaled_boxes[:, 1:3], 0)\n","          scaled_boxes = scaled_boxes.squeeze()\n","\n","          for box_idx in range(scaled_boxes.shape[0]):\n","            box = scaled_boxes[box_idx]\n","            temp_Positive_Token = image[:,int(box[1]):int(box[3]),int(box[2]):int(box[4])]\n","            temp_Positive_Token = temp_Positive_Token.unsqueeze(0)\n","            if temp_Positive_Token.shape[2]!=0 and temp_Positive_Token.shape[3]!=0:\n","              temp_Positive_Token = F.interpolate(temp_Positive_Token, size=(PATCH_SIZE[0], PATCH_SIZE[1]), mode='bilinear')\n","              Positive_Token = torch.cat((Positive_Token, temp_Positive_Token))\n","\n","        if torch.cuda.is_available():\n","            image = image.cuda()\n","            GT_density = GT_density.cuda()\n","            Positive_Token = Positive_Token.cuda()\n","\n","        # Feed to the Network\n","        # Reshape the image to get int patch num\n","        image = image.unsqueeze(0)\n","        if image.shape[-1] % PATCH_SIZE[0] != 0 or image.shape[-2] % PATCH_SIZE[0] != 0:\n","          new_h = (image.shape[-2] // PATCH_SIZE[0]) * 16\n","          new_w = (image.shape[-1] // PATCH_SIZE[1]) * 16\n","          image = F.interpolate(image, size=(new_h, new_w), mode='bilinear')\n","\n","        gt_cnt = dots.shape[0]\n","        out_density = model(image, Positive_Token)\n","\n","        #\n","        GT_density = F.interpolate(GT_density, size=(out_density.shape[2], out_density.shape[3]), mode='bilinear')\n","        GT_density = GT_density*(gt_cnt/GT_density.sum())\n","\n","        #Loss 1\n","        loss = criterion(out_density, GT_density) + 1e-3 * (out_density.sum() - gt_cnt) ** 2\n","\n","        #Loss 2\n","        #loss = criterion(out_density, GT_density)\n","\n","        train_loss += loss.item()\n","\n","        #Error\n","        rec_output = np.maximum(out_density.detach().cpu(), 0)\n","        pred_cnt = rec_output.sum().item()\n","        err = abs(gt_cnt - pred_cnt)\n","        train_mae += err\n","        train_rmse += (err**2)\n","        SAE += err\n","        SSE += err**2\n","        loss.backward()\n","        endtime = datetime.datetime.now()\n","      optimizer.step()\n","    print('TRAIN MAE: {:6.2f}, TRAIN RMSE: {:6.2f}, Train Loss: {}, Running Time:{}'.format(SAE/cnt, (SSE/cnt)**0.5, train_loss/cnt, endtime - starttime))\n","    MAE = SAE/cnt\n","    RMSE = (SSE/cnt)**0.5\n","    TRAIN_LOSS = train_loss/cnt\n","    return MAE, RMSE, TRAIN_LOSS\n","\n","def val():\n","    SSE = 0\n","    SAE = 0\n","    train_mae = 0\n","    train_rmse = 0\n","    train_loss = 0\n","    cnt = 0\n","    starttime = datetime.datetime.now()\n","    for idx_batch in val_loader:\n","      if debug:\n","          if cnt > 10:\n","            break\n","      for idx in idx_batch:\n","        cnt += 1\n","        val_sample = val_dataset[idx]\n","        im_id, image, boxes, dots, bg_mask_img, density = val_sample['im_id'], val_sample['image'], val_sample['boxes'], val_sample['dots'], val_sample['bg_mask_img'], val_sample['gt_density']\n","        sample = {'image': image, 'lines_boxes': boxes, 'gt_density': density}\n","        sample = TransformTrain(sample)\n","        image, boxes, GT_density = sample['image'].to(device), sample['boxes'].to(device), sample['gt_density'].to(device)\n","        boxes = boxes.squeeze()\n","\n","        #Get Positive Token Normal Scale\n","        Positive_Token = None\n","        for box_idx in range(boxes.shape[0]):\n","          box = boxes[box_idx]\n","          temp_Positive_Token = image[:,int(box[1]):int(box[3]),int(box[2]):int(box[4])]\n","          temp_Positive_Token = temp_Positive_Token.unsqueeze(0)\n","          temp_Positive_Token = F.interpolate(temp_Positive_Token, size=(PATCH_SIZE[0], PATCH_SIZE[1]), mode='bilinear')\n","          if Positive_Token is None:\n","            Positive_Token = temp_Positive_Token\n","          else:\n","            Positive_Token = torch.cat((Positive_Token, temp_Positive_Token))\n","\n","        #Get scaling pos token\n","        scaling_para = [0.8, 0.9, 1/0.9, 1/0.8]\n","        for scaling in scaling_para:\n","          scaled_boxes = boxes / scaling\n","          scaled_boxes = scaled_boxes.squeeze()\n","          scaled_boxes[:, 1:3] = torch.floor(scaled_boxes[:, 1:3])\n","          scaled_boxes[:, 3:5] = torch.ceil(scaled_boxes[:, 3:5])\n","          scaled_boxes[:, 3:5] = scaled_boxes[:, 3:5] + 1\n","          scaled_boxes[:, 3] = torch.clamp_max(scaled_boxes[:, 3], image.shape[1] - 1)\n","          scaled_boxes[:, 4] = torch.clamp_max(scaled_boxes[:, 4], image.shape[2] - 1)\n","          scaled_boxes[:, 1:3] = torch.clamp_min(scaled_boxes[:, 1:3], 0)\n","          scaled_boxes = scaled_boxes.squeeze()\n","\n","          for box_idx in range(scaled_boxes.shape[0]):\n","            box = scaled_boxes[box_idx]\n","            temp_Positive_Token = image[:,int(box[1]):int(box[3]),int(box[2]):int(box[4])]\n","            temp_Positive_Token = temp_Positive_Token.unsqueeze(0)\n","            if temp_Positive_Token.shape[2]!=0 and temp_Positive_Token.shape[3]!=0:\n","              temp_Positive_Token = F.interpolate(temp_Positive_Token, size=(PATCH_SIZE[0], PATCH_SIZE[1]), mode='bilinear')\n","              Positive_Token = torch.cat((Positive_Token, temp_Positive_Token))\n","\n","        if torch.cuda.is_available():\n","            image = image.cuda()\n","            GT_density = GT_density.cuda()\n","            Positive_Token = Positive_Token.cuda()\n","\n","        #Feed to the Network\n","        #Reshape the image to get int patch num\n","        image = image.unsqueeze(0)\n","        if image.shape[-1] % PATCH_SIZE[0] != 0 or image.shape[-2] % PATCH_SIZE[0] != 0:\n","          new_h = (image.shape[-2] // PATCH_SIZE[0]) * 16\n","          new_w = (image.shape[-1] // PATCH_SIZE[1]) * 16\n","          image = F.interpolate(image, size=(new_h, new_w), mode='bilinear')\n","        gt_cnt = dots.shape[0]\n","        out_density = model(image, Positive_Token)\n","        GT_density = F.interpolate(GT_density, size=(out_density.shape[2], out_density.shape[3]), mode='bilinear')\n","        GT_density = GT_density*(gt_cnt/GT_density.sum())\n","\n","        #Error\n","        rec_output = np.maximum(out_density.detach().cpu(), 0)\n","        pred_cnt = rec_output.sum().item()\n","        err = abs(gt_cnt - pred_cnt)\n","        train_mae += err\n","        train_rmse += (err**2)\n","        SAE += err\n","        SSE += err**2\n","        endtime = datetime.datetime.now()\n","    print('VAL MAE: {:6.2f}, VAL RMSE: {:6.2f}, Running Time: {}'.format(SAE/cnt, (SSE/cnt)**0.5, endtime - starttime))\n","    MAE = SAE/cnt\n","    RMSE = (SSE/cnt)**0.5\n","    return MAE, RMSE\n","\n","def test():\n","    SSE = 0\n","    SAE = 0\n","    train_mae = 0\n","    train_rmse = 0\n","    train_loss = 0\n","    cnt = 0\n","    starttime = datetime.datetime.now()\n","    for idx_batch in test_loader:\n","      if debug:\n","          if cnt > 10:\n","            break\n","      for idx in idx_batch:\n","        cnt += 1\n","        test_sample = test_dataset[idx]\n","        im_id, image, boxes, dots, bg_mask_img, density = test_sample['im_id'], test_sample['image'], test_sample['boxes'], test_sample['dots'], test_sample['bg_mask_img'], test_sample['gt_density']\n","        sample = {'image': image, 'lines_boxes': boxes, 'gt_density': density}\n","        sample = TransformTrain(sample)\n","        image, boxes, GT_density = sample['image'].to(device), sample['boxes'].to(device), sample['gt_density'].to(device)\n","        boxes = boxes.squeeze()\n","\n","        #Get Positive Token Normal Scale\n","        Positive_Token = None\n","        for box_idx in range(boxes.shape[0]):\n","          box = boxes[box_idx]\n","          temp_Positive_Token = image[:,int(box[1]):int(box[3]),int(box[2]):int(box[4])]\n","          temp_Positive_Token = temp_Positive_Token.unsqueeze(0)\n","          temp_Positive_Token = F.interpolate(temp_Positive_Token, size=(PATCH_SIZE[0], PATCH_SIZE[1]), mode='bilinear')\n","          if Positive_Token is None:\n","            Positive_Token = temp_Positive_Token\n","          else:\n","            Positive_Token = torch.cat((Positive_Token, temp_Positive_Token))\n","\n","        #Get scaling pos token\n","        scaling_para = [0.8, 0.9, 1/0.9, 1/0.8]\n","        for scaling in scaling_para:\n","          scaled_boxes = boxes / scaling\n","          scaled_boxes = scaled_boxes.squeeze()\n","          scaled_boxes[:, 1:3] = torch.floor(scaled_boxes[:, 1:3])\n","          scaled_boxes[:, 3:5] = torch.ceil(scaled_boxes[:, 3:5])\n","          scaled_boxes[:, 3:5] = scaled_boxes[:, 3:5] + 1\n","          scaled_boxes[:, 3] = torch.clamp_max(scaled_boxes[:, 3], image.shape[1] - 1)\n","          scaled_boxes[:, 4] = torch.clamp_max(scaled_boxes[:, 4], image.shape[2] - 1)\n","          scaled_boxes[:, 1:3] = torch.clamp_min(scaled_boxes[:, 1:3], 0)\n","          scaled_boxes = scaled_boxes.squeeze()\n","\n","          for box_idx in range(scaled_boxes.shape[0]):\n","            box = scaled_boxes[box_idx]\n","            temp_Positive_Token = image[:,int(box[1]):int(box[3]),int(box[2]):int(box[4])]\n","            temp_Positive_Token = temp_Positive_Token.unsqueeze(0)\n","            if temp_Positive_Token.shape[2]!=0 and temp_Positive_Token.shape[3]!=0:\n","              temp_Positive_Token = F.interpolate(temp_Positive_Token, size=(PATCH_SIZE[0], PATCH_SIZE[1]), mode='bilinear')\n","              Positive_Token = torch.cat((Positive_Token, temp_Positive_Token))\n","\n","        if torch.cuda.is_available():\n","            image = image.cuda()\n","            GT_density = GT_density.cuda()\n","            Positive_Token = Positive_Token.cuda()\n","\n","        #Feed to the Network\n","        #Reshape the image to get int patch num\n","        image = image.unsqueeze(0)\n","        if image.shape[-1] % PATCH_SIZE[0] != 0 or image.shape[-2] % PATCH_SIZE[0] != 0:\n","          new_h = (image.shape[-2] // PATCH_SIZE[0]) * 16\n","          new_w = (image.shape[-1] // PATCH_SIZE[1]) * 16\n","          image = F.interpolate(image, size=(new_h, new_w), mode='bilinear')\n","        gt_cnt = dots.shape[0]\n","        out_density = model(image, Positive_Token)\n","        GT_density = F.interpolate(GT_density, size=(out_density.shape[2], out_density.shape[3]), mode='bilinear')\n","        GT_density = GT_density*(gt_cnt/GT_density.sum())\n","\n","        #Error\n","        rec_output = np.maximum(out_density.detach().cpu(), 0)\n","        pred_cnt = rec_output.sum().item()\n","        err = abs(gt_cnt - pred_cnt)\n","        train_mae += err\n","        train_rmse += (err**2)\n","        SAE += err\n","        SSE += err**2\n","        endtime = datetime.datetime.now()\n","    print('TEST MAE: {:6.2f}, TEST RMSE: {:6.2f}, Running Time: {}'.format(SAE/cnt, (SSE/cnt)**0.5, endtime - starttime))\n","    MAE = SAE/cnt\n","    RMSE = (SSE/cnt)**0.5\n","    return MAE, RMSE\n","\n","\n","EPOCH = 150\n","debug = True\n","stats = list()\n","Log_Save_dir = './'\n","best_mae, best_rmse = 1e7, 1e7\n","# Train\n","for epoch in range(EPOCH):\n","  if debug:\n","    if epoch > 5:\n","      break\n","  model.train()\n","  train_mae, train_rmse, train_loss = train()\n","  model.eval()\n","  val_mae, val_rmse = val()\n","  stats.append((train_loss, train_mae, train_rmse, val_mae, val_rmse))\n","  stats_file = os.path.join(Log_Save_dir, \"Log.txt\")\n","  with open(stats_file, 'w') as f:\n","      for s in stats:\n","          f.write(\"%s\\n\" % ','.join([str(x) for x in s]))\n","  if best_mae >= val_mae:\n","    best_mae = val_mae\n","    best_rmse = val_rmse\n","    model_save_path = os.path.join(Log_Save_dir, \"Best_Model.pth\")\n","    torch.save(model.state_dict(), model_save_path)\n","  if (epoch + 1) % 5 == 0:\n","    print(\"\\033[1;31;47mEpoch {}, Avg. Epoch Loss: {} Train MAE: {} Train RMSE: {} Val MAE: {} Val RMSE: {} Best Val MAE: {} Best Val RMSE: {} \\033[0m\".format(\n","              epoch + 1, stats[-1][0], stats[-1][1], stats[-1][2], stats[-1][3], stats[-1][4], best_mae, best_rmse))\n","\n","# Test\n","model.load_state_dict(torch.load(model_save_path))\n","model.eval()\n","test_mae, test_rmse = test()\n","print(\"\\033[1;33;44mTest MAE {}, Test RMSE {} \\033[0m\".format(test_mae, test_rmse))"],"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-1951eef05d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m   \u001b[0mtrain_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m   \u001b[0mval_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-1951eef05d1d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"]}],"execution_count":null},{"cell_type":"markdown","source":["PART IV: (20 points)\n","\n","What are your results like? Why do you think the model doesn't perform better? How would you improve this model's accuracy?\n","\n","Answer these questions using what you have learned from this course and\n","material provided above.\n","\n","BONUS: (30 points)\n","\n","Modify this network according to your explanation, run it again.\n","How does the loss value change compared with the original model?\n","\n","When you do this part, save the code and the model in another file named\n","CV_2022FALL_hw5.bonus.ipynb\n","and attach a graph of your model, explaining the input and output parts clearly."],"metadata":{"id":"-y-tODHtTKzC"}},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"U_FmkJhWe55I"},"source":["## Submission guidelines\n","---\n","We will grade your homework based on your submitted notebook file. We will check the notebook for both results and code. Please make sure you run your code and print out the results in the notebook before submitting (we expect to see the results before running your code by ourselves.)\n","\n","You submit your homework by first creating a ***google shared link*** of a folder for your homework (described below), and put that link into the ***text submission section*** of your homework submission on Blackboard. ([How to submit your link?](https://drive.google.com/file/d/16-FlPSiu5n-pRezLfcbAvgYxXtGtrs16))\n","\n","To generate the ***google shared link***, first create a folder named ***Surname_Givenname_SBUID_hw**** in your Google Drive with your CS account (or your SBU account if you don't have a CS account). The structure of the files in the folder should be exactly the same as the one you downloaded. For instance in this homework:\n","\n","```\n","Surname_Givenname_SBUID_hw5\n","        |---CSE527-22F-HW5.ipynb\n","```\n","Note that this folder should be in your Google Drive with your account.\n","\n","Then right click this folder, click ***Get shareable link***, in the People textfield, enter the TAs' email: ***haoyuwu@cs.stonybrook.edu*** and ***vhnguyen@cs.stonybrook.edu***. Make sure that the TAs who have the link **can edit**, ***not just*** **can view**, and also **uncheck** the **Notify people** box. ([How to share link?](https://drive.google.com/file/d/17R6j6yE8_8vXioOB3nNvbEPzxcI-rr_H) )\n","\n","***IMPORTANT: Please do not make any modification to the folder and its files after the submission deadline***. (All modifications can be seen by the TAs via the revision history.) Note that in google colab, we will only grade the version of the code right before the timestamp of the submission made in blackboard.\n","\n","The input and output paths are predefined and **DO NOT** change them, (we assume that 'Surname_Givenname_SBUID_hw1' is your working directory, and all the paths are relative to this directory).  The image read and write functions are already written for you. All you need to do is to fill in the blanks as indicated to generate proper outputs.\n","\n","\n","-- DO NOT change the folder structure, please just fill in the blanks. <br>\n","\n","You are encouraged to post and answer questions on Piazza. Based on the amount of email that we have received in past years, there might be delays in replying to personal emails. Please ask questions on Piazza and send emails only for personal issues.\n","\n","If you alter the folder structures, the grading of your homework will be significantly delayed and possibly penalized.\n","\n","Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work.\n","\n","Late submission penalty: <br>\n","There will be a 10% penalty per day for late submission. However, you will have 4 days throughout the whole semester to submit late without penalty. Note that the grace period is calculated by days instead of hours. If you submit the homework one minute after the deadline, one late day will be counted. Likewise, if you submit one minute after the deadline, the 10% penaly will be imposed if not using the grace period.\n"]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"},"colab":{"provenance":[]},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}